由于时间关系，本项目完成功能为：

- 单次生成模式
- 终端对话模式
- 兼容OpenAI的Server模式

项目运行方式：

```shell
# 查看项目运行参数
cargo run -r -- --help
```
运行输出：
```output
Usage: learning-lm-rust --model-path <MODEL_PATH> <COMMAND>

Commands:
  once    单次生成模式
  chat    终端对话模式
  server  兼容OpenAI的Server模式
  help    Print this message or the help of the given subcommand(s)

Options:
  -m, --model-path <MODEL_PATH>  模型路径
  -h, --help                     Print help
  -V, --version                  Print version
```

### 单次生成模式
```shell
# 查看运行参数
cargo run -r -- -m ./models/chat once -h
```
运行输出：
```output
单次生成模式

Usage: learning-lm-rust --model-path <MODEL_PATH> once [OPTIONS] --prompt <PROMPT>

Options:
      --prompt <PROMPT>            输入提示词
  -m, --max-length <MAX_LENGTH>    输出长度限制 [default: 512]
  -k, --top-k <TOP_K>              Top-k 输出 [default: 10]
  -t, --temperature <TEMPERATURE>  Temperature 控制 [default: 1]
  -p, --top-p <TOP_P>              Top-p 控制 [default: 0.9]
      --stream                     流式输出
  -h, --help                       Print help
```

### 终端对话模式
```shell
# 查看运行参数
cargo run -r -- -m ./models/story once -h
```
运行输出：
```output
终端对话模式

Usage: learning-lm-rust --model-path <MODEL_PATH> chat [OPTIONS]

Options:
      --system-prompt <SYSTEM_PROMPT>  系统提示
  -m, --max-length <MAX_LENGTH>        输出长度限制 [default: 512]
  -k, --top-k <TOP_K>                  Top-k 输出 [default: 10]
  -t, --temperature <TEMPERATURE>      Temperature 控制 [default: 1]
  -p, --top-p <TOP_P>                  Top-p 控制 [default: 0.9]
      --stream                         流式输出
      --template <TEMPLATE>            对话模板 [default: chatml]
  -h, --help                           Print help
```

### 终端对话模式
```shell
# 查看运行参数
cargo run -r -- -m ./models/chat chat -h
```
运行输出：
```output
终端对话模式

Usage: learning-lm-rust --model-path <MODEL_PATH> chat [OPTIONS]

Options:
      --system-prompt <SYSTEM_PROMPT>  系统提示
  -m, --max-length <MAX_LENGTH>        输出长度限制 [default: 512]
  -k, --top-k <TOP_K>                  Top-k 输出 [default: 10]
  -t, --temperature <TEMPERATURE>      Temperature 控制 [default: 1]
  -p, --top-p <TOP_P>                  Top-p 控制 [default: 0.9]
      --stream                         流式输出
      --template <TEMPLATE>            对话模板 [default: chatml]
  -h, --help                           Print help
```

### 兼容OpenAI的Server模式
项目jinja2模板使用了tera库，该库对tokenizer_config.json中的chat_template有一定问题，故暂时使用在templates目录下*.jinja2模板。
```shell
# 查看运行参数
cargo run -r -- -m ./models/chat server -h
```

运行输出：
```output
兼容OpenAI的Server模式

Usage: learning-lm-rust --model-path <MODEL_PATH> server [OPTIONS]

Options:
      --port <PORT>          端口号 [default: 8000]
      --host <HOST>          host [default: 127.0.0.1]
      --template <TEMPLATE>  对话模板 [default: chatml]
  -h, --help                 Print help
```

